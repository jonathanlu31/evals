#!/bin/bash

# Make sure to start the vLLM server using serve_vllm.sh first

set -x

MODEL_DIR=${1}
PORT=${2:-8000}

cd "$(dirname "$0")"

python gen_api_answer.py --model $MODEL_DIR --openai-api-base http://localhost:${PORT}/v1 --parallel 20
