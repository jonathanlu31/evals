#!/bin/bash

set -x

MODEL_DIR=${1}

# Start vLLM server in the background
vllm serve $MODEL_DIR --dtype auto --max-model-len 4096 &
VLLM_PID=$!

# Wait for server to become ready (simple check loop)
until curl --silent --fail http://localhost:8000/metrics > /dev/null; do
  echo "Waiting for vLLM server to be ready..."
  sleep 10
done

# Run the benchmark
python gen_api_answer.py --model $MODEL_DIR --openai-api-base http://localhost:8000/v1 --parallel 50

# Kill the vLLM server
kill -2 $VLLM_PID
wait $VLLM_PID
