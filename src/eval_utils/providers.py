"""Modified from torchllms"""

import concurrent
import os
from abc import ABC, abstractmethod
from typing import List

from dotenv import load_dotenv
from rich.progress import track

load_dotenv()


class BaseProvider(ABC):
    @abstractmethod
    def generate(
        self,
        conversations: List[List[dict[str, str]]],
        **kwargs,
    ) -> List[List[dict[str, str]]]:
        """Generate completions for a batch of conversations.

        Extends conversations in-place with assistant responses appended. Should return
        empty strings instead of raising exceptions.

        Args:
            conversations: list of lists of messages. Each message is a dictionary with
                keys "role" and "content".
            kwargs: additional keyword arguments for the model call.
        """
        pass


class vLLM(BaseProvider):
    def __init__(
        self,
        model_path,
        max_model_len=4096,
        tensor_parallel_size=None,
        enforce_eager=True,
        model_kwargs={},
    ):
        from transformers import AutoTokenizer
        from vllm import LLM, SamplingParams

        self.SamplingParams = SamplingParams
        self.model = LLM(
            model_path,
            max_model_len=max_model_len,
            tensor_parallel_size=tensor_parallel_size,
            enforce_eager=enforce_eager,
            **model_kwargs,
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

    def generate(
        self,
        conversations: List[List[dict[str, str]]],
        strip_response: bool = False,
        **kwargs,
    ) -> List[List[dict[str, str]]]:
        """vLLM uses tokenizer settings to determine when to stop generation."""

        if "max_new_tokens" in kwargs:
            kwargs["max_tokens"] = kwargs.pop("max_new_tokens")

        sampling_params = self.SamplingParams(**kwargs)

        prompts = []
        for conv in conversations:
            tokens = self.tokenizer.apply_chat_template(
                conv, tokenize=True, add_generation_prompt=True
            )
            prompts.append({"prompt_token_ids": tokens})

        # LLM.chat() is buggy: https://github.com/vllm-project/vllm/issues/9519
        outputs = self.model.generate(
            prompts=prompts,
            sampling_params=sampling_params,
        )

        for messages, out in zip(conversations, outputs):
            response = out.outputs[0].text
            if strip_response:
                response = response.strip()
            messages.append({"role": "assistant", "content": response})

        return conversations


class OpenAI(BaseProvider):
    def __init__(self, model: str, concurrency: int = 20):
        import openai

        self.model = model
        self.concurrency = concurrency

        if "gemini" in model.lower():
            api_key = os.getenv("GEMINI_API_KEY")
            base_url = "https://generativelanguage.googleapis.com/v1beta/openai/"
        elif "deepseek" in model.lower():
            api_key = os.environ.get("TOGETHER_API_KEY")
            base_url = "https://api.together.xyz/v1"
        else:
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = None

        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url,
            max_retries=3,
        )

    def _get_completion(self, messages, **kwargs):
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                **kwargs,
            )
        except Exception as e:
            print(e)
            return "missing"

        if len(response.choices) > 0 and response.choices[0].message.content:
            return response.choices[0].message.content

        return "missing"

    def generate(
        self,
        conversations: List[List[dict[str, str]]],
        **kwargs,
    ) -> List[List[dict[str, str]]]:
        if "max_new_tokens" in kwargs:
            kwargs["max_tokens"] = kwargs.pop("max_new_tokens")

        with concurrent.futures.ThreadPoolExecutor(
            max_workers=self.concurrency
        ) as executor:
            threads = {}
            for messages in conversations:
                t = executor.submit(
                    self._get_completion,
                    messages=messages,
                    **kwargs,
                )
                threads[t] = messages

            try:
                for t in track(
                    concurrent.futures.as_completed(threads),
                    description="[cyan]Calling OpenAI API:",
                    total=len(threads),
                ):
                    response = t.result()
                    threads[t].append({"role": "assistant", "content": response})
            except KeyboardInterrupt:
                executor._threads.clear()
                concurrent.futures.thread._threads_queues.clear()
                print(
                    "Keyboard interrupt received. Shutting down and saving results..."
                )

        return conversations


# class Google(BaseProvider):
#     SAFETY_SETTINGS = [
#         {
#             "category": c,
#             "threshold": "block_none",
#         }
#         for c in ["harassment", "hate", "sex", "danger"]
#     ]

#     def __init__(self, model: str, concurrency: int = 20):
#         import google.generativeai as genai
#         from google.api_core import retry
#         from google.generativeai.types import RequestOptions

#         self.REQUEST_OPTIONS = RequestOptions(
#             retry=retry.Retry(initial=10, multiplier=2, maximum=60, timeout=300),
#         )

#         genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
#         self.concurrency = concurrency
#         self.model = model

#         def _get_completion(messages, generation_config):
#             contents, system = self.encode(messages)
#             model = genai.GenerativeModel(self.model, system_instruction=system)
#             try:
#                 response = model.generate_content(
#                     contents,
#                     request_options=self.REQUEST_OPTIONS,
#                     safety_settings=self.SAFETY_SETTINGS,
#                     generation_config=generation_config,
#                 )
#             except Exception as e:
#                 print(e)
#                 return "missing"

#             if (
#                 len(response.candidates) > 0
#                 and len(response.candidates[0].content.parts) > 0
#             ):
#                 return response.text

#             return "missing"

#         self._get_completion = _get_completion

#     def encode(self, messages: List[dict[str, str]]):
#         system = None
#         encoded = []
#         for m in messages:
#             if m["role"] == "user":
#                 encoded.append({"role": "user", "parts": [m["content"]]})
#             elif m["role"] == "assistant":
#                 encoded.append({"role": "model", "parts": [m["content"]]})
#             elif m["role"] == "system":
#                 system = m["content"]
#         return encoded, system

#     def generate(
#         self,
#         conversations: List[List[dict[str, str]]],
#         **kwargs,
#     ) -> List[List[dict[str, str]]]:
#         if "max_new_tokens" in kwargs:
#             kwargs["max_output_tokens"] = kwargs.pop("max_new_tokens")

#         with concurrent.futures.ThreadPoolExecutor(
#             max_workers=self.concurrency
#         ) as executor:
#             threads = {}
#             for messages in conversations:
#                 t = executor.submit(
#                     self._get_completion,
#                     messages=messages,
#                     generation_config=kwargs,
#                 )
#                 threads[t] = messages

#             try:
#                 for t in track(
#                     concurrent.futures.as_completed(threads),
#                     description="[cyan]Calling Google API:",
#                     total=len(threads),
#                 ):
#                     threads[t].append({"role": "assistant", "content": t.result()})
#             except KeyboardInterrupt:
#                 executor._threads.clear()
#                 concurrent.futures.thread._threads_queues.clear()
#                 print(
#                     "Keyboard interrupt received. Shutting down and saving results..."
#                 )

#         return conversations


PROVIDERS = {
    "vllm": vLLM,
    "openai": OpenAI,
    # "google": Google,
}
