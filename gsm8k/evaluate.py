import dataclasses
import json
import os
import random
import argparse
from collections import defaultdict
from pathlib import Path
from typing import Dict, List

@dataclasses.dataclass
class OutputExample:
    key: int
    question: str
    model_answer: str
    ground_truth: str
    is_correct: bool

def extract_final_answer(text):
    import re
    match = re.search(r"####\s*(-?\d+(?:\.\d+)?)", text)
    return match.group(1).strip() if match else None

def compute_scores(correct_list):
    acc = sum(correct_list) / len(correct_list)
    return {"accuracy": acc}

def write_outputs(path, outputs):
    with open(path, "w") as f:
        for ex in outputs:
            f.write(json.dumps(dataclasses.asdict(ex)) + "\n")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_data", required=True)
    parser.add_argument("--input_response_data", required=True)
    parser.add_argument("--output_dir", required=True)
    parser.add_argument("--bootstrap", type=int, default=0)
    args = parser.parse_args()

    with open(args.input_data) as f:
        prompts = [json.loads(line) for line in f]
    with open(args.input_response_data) as f:
        generations = [json.loads(line) for line in f]

    results = []
    correct_flags = []

    for i, (p, g) in enumerate(zip(prompts, generations)):
        gold = extract_final_answer(p["answer"])
        pred = extract_final_answer(g["text"])
        is_correct = pred == gold
        results.append(OutputExample(
            key=i,
            question=p["question"],
            model_answer=g["text"],
            ground_truth=p["answer"],
            is_correct=is_correct
        ))
        correct_flags.append(is_correct)

    os.makedirs(args.output_dir, exist_ok=True)
    write_outputs(Path(args.output_dir) / "eval_results.jsonl", results)

    scores = compute_scores(correct_flags)
    print("Accuracy:", f"{scores['accuracy']:.2%}")

    if args.bootstrap > 0:
        accs = []
        for _ in range(args.bootstrap):
            sample = random.choices(correct_flags, k=len(correct_flags))
            accs.append(sum(sample)/len(sample))
        accs.sort()
        lower = accs[int(0.025 * len(accs))]
        upper = accs[int(0.975 * len(accs))]
        print(f"Bootstrap CI (95%): {lower:.2%} - {upper:.2%}")

if __name__ == "__main__":
    main()