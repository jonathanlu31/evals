# Evals

## Installation

```bash
pip install .
pip install ".[ifeval]"
```

## Usage

For each benchmark, use the provided scripts to run them

For larger models, use vLLM server

```bash
bash serve_vllm.sh <model_dir> <num_gpus>
```

For reasoning models, you must use vLLM server, otherwise the response parsing will not work correctly and the evaluations will break.

Then, use `run_all_vllm.sh` to run all the evals and `read_all.sh` to read the results.

```bash
bash run_all_vllm.sh <model_dir>
bash read_all.sh <model_name>
```

### GPQA-Diamond

To run GPQA-Diamond on reasoning models with the configurations from DeepSeek R1, you need to set `MAX_MODEL_LEN` to 32,768.
